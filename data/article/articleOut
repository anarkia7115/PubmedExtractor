26390521|t|How should I treat a significant and inoperable left main coronary atherosclerotic disease (LMCAD) in the setting of a severely depressed left ventricular systolic function and severe aorto-iliac atherosclerotic disease?
26390521|a|-No abstract-

26390520|t|Sympathetic denervation of heart and kidney induces similar effects on ventricular electrophysiological properties.
26390520|a|<AbstractText Label="AIMS" NlmCategory="OBJECTIVE">We aimed to investigate whether sympathetic denervation of the heart and kidney had similar effects on ventricular effective refractory period (ERP) and action potential duration (APD) restitution properties in a canine model. </AbstractText><AbstractText Label="METHODS AND RESULTS" NlmCategory="RESULTS">Twenty-four anaesthetised open-chest dogs (17-20 kg) were assigned to a sham operation group (n=8), a cardiac sympathetic denervation group (CSD, n=8) or a renal sympathetic denervation group (RSD, n=8). CSD was performed by ablating the caudal half of the LSG and T2-T4 thoracic sympathetic ganglia, while RSD was performed by ablating four sites on the adventitial surface of each renal artery. The ventricular electrophysiological properties were determined at four time points: baseline, 0 min, 30 min, and 60 min after interventions. The results showed that, when compared to the control group at the time point of 60 min after interventions, both CSD and RSD significantly reduced heart rate, prolonged the QT interval and ventricular ERP and APD, decreased the ERP dispersion and the slopes of APD restitution curves, and suppressed the APD alternans without affecting blood pressure and corrected QT interval. However, there were no significant differences in these parameters between CSD and RSD groups at the same time point. </AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">This study showed that sympathetic denervation of the heart and kidney induced similar electrophysiological effects in ventricles. </AbstractText>

26390519|t|Unintentional sealing of the mitral valve with a second MitraClip. "Once bitten, twice shy".
26390519|a|-No abstract-

26390518|t|Clinical impact of coronary protection during transcatheter aortic valve implantation: first reported series of patients.
26390518|a|<AbstractText Label="AIMS" NlmCategory="OBJECTIVE">Coronary protection with guidewires and an undeployed coronary balloon or stent positioned in the coronary artery is a pre-emptive technique to manage coronary obstruction during transcatheter aortic valve implantation (TAVI). We investigated the feasibility and safety of left main (LM) protection during TAVI. </AbstractText><AbstractText Label="METHODS AND RESULTS" NlmCategory="RESULTS">Twenty-five out of 623 patients who underwent TAVI at our institute were deemed to be at increased risk of LM compromise mainly due to a low LM ostium height, significant LM disease or a previous bioprosthetic valve. A pre-emptive LM protection technique was therefore used in these cases. Five patients (20%) had pre-TAVI significant non-revascularised LM stenosis, and four patients (16%) had a prior LM ostial stent without pre-TAVI in-stent restenosis. Twelve patients had extremely low LM height (mean height 6.7±2.4 mm; range 1.1-8.9 mm). Seven patients (25%) had valve-in-valve (VIV) procedures. LM compromise occurred in five out of 25 cases; all were treated successfully with emergency LM stenting. Nine patients underwent successful planned LM procedures following TAVI. </AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">The LM protection technique should be considered in patients deemed to be at increased risk of LM compromise. This was found to be helpful in the prompt diagnosis and treatment of LM compromise following TAVI. </AbstractText>

26390517|t|Comparison of neointimal coverage between everolimus-eluting stents and sirolimus-eluting stents: an optical coherence tomography substudy of the RESET (Randomized Evaluation of Sirolimus-eluting versus Everolimus-eluting stent Trial).
26390517|a|<AbstractText Label="AIMS" NlmCategory="OBJECTIVE">The aim of the present study was to compare vascular healing response between the first-generation sirolimus-eluting stent (SES) and the second-generation everolimus-eluting stent (EES) by using optical coherence tomography (OCT). </AbstractText><AbstractText Label="METHODS AND RESULTS" NlmCategory="RESULTS">The RESET was a prospective, multicentre, randomised, open-label trial comparing EES and SES. Of the 3,197 patients enrolled in the RESET, nine-month follow-up OCT after stent implantation was performed in 100 patients (48 EES-treated lesions in 44 patients and 62 SES-treated lesions in 56 patients), thus constituting the OCT substudy population. The percentage of uncovered struts per lesion (8±15% vs. 14±19%, p=0.031) and cross-sections with >30% uncovered struts per lesion (6±14% vs. 18±29%, p=0.009) was significantly lower in EES compared with SES. The frequency of DES-treated lesions with incomplete stent apposition (8 [17%] vs. 29 [47%], p<0.001) was significantly lower in EES compared with SES. Intra-stent thrombus was comparably observed between EES and SES (1 [2%] vs. 5 [8%], p=0.229). </AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">Compared with SES, EES was associated with a favourable vascular response at nine months after stent implantation. </AbstractText>

26390516|t|A comparison of multivessel and culprit vessel percutaneous coronary intervention in non-ST-segment elevation acute coronary syndrome patients with multivessel disease: a meta-analysis.
26390516|a|<AbstractText Label="AIMS" NlmCategory="OBJECTIVE">Percutaneous revascularisation triage has not been evaluated in randomised controlled trials of patients with non-ST-segment elevation acute coronary syndromes (NSTE-ACS) and multivessel disease. As a result, current guidelines are not available. The objective of our meta-analysis was to investigate the use of percutaneous coronary intervention (PCI) in culprit and non-culprit vessels. </AbstractText><AbstractText Label="METHODS AND RESULTS" NlmCategory="RESULTS">We undertook a meta-analysis of controlled studies where patients were assigned to multivessel PCI or culprit vessel PCI. Summary odds ratios (OR) for all-cause mortality, myocardial infarction, unplanned revascularisation and major adverse cardiac events (MACE) were calculated using random- or fixed-effect models. Six registry studies (n=5,414) were included in this meta-analysis. There was no difference in the rate of mortality (OR, 0.85; 95% CI: 0.70 to 1.04; p=0.114) or myocardial infarction (OR, 0.75; 95% CI: 0.43 to 1.32; p=0.319) between the two treatment groups. Multivessel PCI may decrease long-term MACE (OR, 0.69; 95% CI: 0.51 to 0.93; p=0.015) and unplanned revascularisation (OR, 0.64; 95% CI: 0.45 to 93; p=0.018) compared with culprit vessel PCI. </AbstractText><AbstractText Label="CONCLUSIONS" NlmCategory="CONCLUSIONS">No significant difference was demonstrated in the long-term risk of myocardial infarction and mortality between multivessel PCI and culprit vessel PCI. Therefore, multivessel PCI may be a safe and reasonable option for NSTE-ACS patients with multivessel disease. </AbstractText>

26390515|t|Further refining the technique: new concepts in TAVI research.
26390515|a|-No abstract-

26390514|t|Time, space and leaps of faith.
26390514|a|-No abstract-

26390513|t|The era of a new generation of interventionalists: a hybrid of a surgeon and an interventionalist?
26390513|a|-No abstract-

26390512|t|Early hypoattenuated leaflet thickening and restricted leaflet motion of a Lotus transcatheter heart valve detected by 4D computed tomography angiography.
26390512|a|-No abstract-

26390511|t|Treatment of pure aortic insufficiency after aortic stentless valve implantation: increasing safety with a transcatheter inflatable fully repositionable valve.
26390511|a|-No abstract-

26390510|t|CoreValve Evolut R implantation as valve-in-valve in an Edwards SAPIEN 3 to treat paravalvular regurgitation.
26390510|a|-No abstract-

26390509|t|Reversal of flow between serial bifurcation lesions: insights from computational fluid dynamic analysis in a population-based phantom model.
26390509|a|-No abstract-

26390508|t|Impact of the orbital atherectomy system on a coronary calcified lesion: quantitative analysis by light attenuation in optical coherence tomography.
26390508|a|-No abstract-

26390507|t|Synchronization of a Class of Switched Neural Networks with Time-Varying Delays via Nonlinear Feedback Control.
26390507|a|<AbstractText Label="null" NlmCategory="UNLABELLED">This paper is concerned with the synchronization problem for a class of switched neural networks (SNNs) with time-varying delays. First, a new crucial lemma which includes and extends the classical exponential stability theorem is constructed. Then by using the lemma, new algebraic criteria of Ψ-type synchronization (synchronization with general decay rate) for SNNs are established via the designed nonlinear feedback control. The Ψ-type synchronization which is in a general framework is obtained by introducing a Ψ-type function. It contains exponential synchronization, polynomial synchronization, and other synchronization as its special cases. The results of this paper are general, and they also complement and extend some previous results. Finally, numerical simulations are carried out to demonstrate the effectiveness of the obtained results. </AbstractText>

26390506|t|Stochastic Opposition-Based Learning Using a Beta Distribution in Differential Evolution.
26390506|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Since it first appeared, differential evolution (DE), one of the most successful evolutionary algorithms, has been studied by many researchers. Theoretical and empirical studies of the parameters and strategies have been conducted, and numerous variants have been proposed. Opposition-based DE (ODE), one of such variants, combines DE with opposition-based learning (OBL) to obtain a high-quality solution with low-computational effort. In this paper, we propose a novel OBL using a beta distribution with partial dimensional change and selection switching and combine it with DE to enhance the convergence speed and searchability. Our proposed algorithm is tested on various test functions and compared with standard DE and other ODE variants. The results indicate that the proposed algorithm outperforms the comparison group, especially in terms of solution accuracy. </AbstractText>

26390505|t|Towards Health Exercise Behavior Change for Teams Using Lifelog Sharing Models.
26390505|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Recent technological trends in mobile/wearable devices and sensors have been enabling an increasing number of people to collect and store their "lifelog" easily in their daily lives. Beyond exercise behavior change of individual users, our research focus is on the behavior change of teams, based on life-logging technologies and lifelog sharing. In this paper, we propose and evaluate six different types of lifelog sharing models among team members for their exercise promotion, leveraging the concepts of "competition" and "collaboration." According to our experimental mobile web application for exercise promotion and an extensive user study conducted with a total of 64 participants over a period of three weeks, the model with "competition" technique resulted in the most effective performance for competitive teams, such as sports teams. </AbstractText>

26390504|t|Electrotactile Augmentation for Carving Guidance.
26390504|a|<AbstractText Label="null" NlmCategory="UNLABELLED">In this study, we presented an efficient and unobtrusive tactile feedback system, which is used to train dental technicians in carving tasks using a wax stick and knife. First, we developed a method for generating performance metrics using a model-based estimation of clearance angles between an object's surface and the carving blade. The calculated clearance angles are compared with desired angles obtained from expert operators. Then, angular errors are presented as tactile cues to the user's finger pads through electrical stimuli at the middle phalanx of the index finger and the thumb. Subsequently, we conducted a feasibility test with novice dental technicians, who showed improvement in initial clearance angles of carving strokes. Moreover, the results showed significant reduction in the occurrence rate of poor-carving when using the proposed system. From these results, we concluded that electrotactile augmentation can provide effective guidance for carving tasks. </AbstractText>

26390503|t|Pareto Optimal Design for Synthetic Biology.
26390503|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Recent advances in synthetic biology call for robust, flexible and efficient in silico optimization methodologies. We present a Pareto design approach for the bi-level optimization problem associated to the overproduction of specific metabolites in Escherichia coli. Our method efficiently explores the high dimensional genetic manipulation space, finding a number of trade-offs between synthetic and biological objectives, hence furnishing a deeper biological insight to the addressed problem and important results for industrial purposes. We demonstrate the computational capabilities of our Pareto-oriented approach comparing it with state-of-the-art heuristics in the overproduction problems of i) 1,4-butanediol, ii) myristoyl-CoA, i ii) malonyl-CoA , iv) acetate and v) succinate. We show that our algorithms are able to gracefully adapt and scale to more complex models and more biologically-relevant simulations of the genetic manipulations allowed. The Results obtained for 1,4-butanediol overproduction significantly outperform results previously obtained, in terms of 1,4-butanediol to biomass formation ratio and knock-out costs. In particular overproduction percentage is of +662.7%, from 1.425 mmolh(-1)gDW(-1) (wild type) to 10.869 mmolh(-1)gDW(-1), with a knockout cost of 6. Whereas, Pareto-optimal designs we have found in fatty acid optimizations strictly dominate the ones obtained by the other methodologies, e.g., biomass and myristoyl-CoA exportation improvement of +21.43% (0.17 h(-1)) and +5.19% (1.62 mmolh(-1)gDW(-1)), respectively. Furthermore CPU time required by our heuristic approach is more than halved. Finally we implement pathway oriented sensitivity analysis, epsilon-dominance analysis and robustness analysis to enhance our biological understanding of the problem and to improve the optimization algorithm capabilities. </AbstractText>

26390502|t|Designing Genetic Feedback Controllers.
26390502|a|<AbstractText Label="null" NlmCategory="UNLABELLED">By incorporating feedback around systems we wish to manipulate, it is possible to improve their performance and robustness properties to meet pre-specified design objectives. For decades control engineers have been successfully implementing feedback controllers for complex mechanical and electrical systems such as aircraft and sports cars. Natural biological systems use feedback extensively for regulation and adaptation but apart from the most basic designs, there is no systematic framework for designing feedback controllers in Synthetic Biology. In this paper we describe how classical approaches from linear control theory can be used to close the loop. This includes the design of genetic circuits using feedback control and the presentation of a biological phase lag controller. </AbstractText>

26390501|t|Neurochemostat: A Neural Interface SoC With Integrated Chemometrics for Closed-Loop Regulation of Brain Dopamine.
26390501|a|<AbstractText Label="null" NlmCategory="UNLABELLED">This paper presents a 3.3×3.2 mm(2) system-on-chip (SoC) fabricated in AMS 0.35 μm 2P/4M CMOS for closed-loop regulation of brain dopamine. The SoC uniquely integrates neurochemical sensing, on-the-fly chemometrics, and feedback-controlled electrical stimulation to realize a "neurochemostat" by maintaining brain levels of electrically evoked dopamine between two user-set thresholds. The SoC incorporates a 90 μW, custom-designed, digital signal processing (DSP) unit for real-time processing of neurochemical data obtained by 400 V/s fast-scan cyclic voltammetry (FSCV) with a carbon-fiber microelectrode (CFM). Specifically, the DSP unit executes a chemometrics algorithm based upon principal component regression (PCR) to resolve in real time electrically evoked brain dopamine levels from pH change and CFM background-current drift, two common interferents encountered using FSCV with a CFM in vivo. Further, the DSP unit directly links the chemically resolved dopamine levels to the activation of the electrical microstimulator in on-off-keying (OOK) fashion. Measured results from benchtop testing, flow injection analysis (FIA), and biological experiments with an anesthetized rat are presented. </AbstractText>

26390500|t|Normally Off ECG SoC With Non-Volatile MCU and Noise Tolerant Heartbeat Detector.
26390500|a|<AbstractText Label="null" NlmCategory="UNLABELLED">This paper describes an electrocardiograph (ECG) monitoring SoC using a non-volatile MCU (NVMCU) and a noise-tolerant instantaneous heartbeat detector. The novelty of this work is the combination of the non-volatile MCU for normally off computing and a noise-tolerant-QRS (heartbeat) detector to achieve both low-power and noise tolerance. To minimize the stand-by current of MCU, a non-volatile flip-flop and a 6T-4C NVRAM are used. Proposed plate-line charge-share and bit-line non-precharge techniques also contribute to mitigate the active power overhead of 6T-4C NVRAM. The proposed accurate heartbeat detector uses coarse-fine autocorrelation and a template matching technique. Accurate heartbeat detection also contributes system-level power reduction because the active ratio of ADC and digital block can be reduced using heartbeat prediction. Measurement results show that the fully integrated ECG-SoC consumes 6.14 μ A including 1.28- μA non-volatile MCU and 0.7- μA heartbeat detector. </AbstractText>

26390499|t|A Digital Realization of Astrocyte and Neural Glial Interactions.
26390499|a|<AbstractText Label="null" NlmCategory="UNLABELLED">The implementation of biological neural networks is a key objective of the neuromorphic research field. Astrocytes are the largest cell population in the brain. With the discovery of calcium wave propagation through astrocyte networks, now it is more evident that neuronal networks alone may not explain functionality of the strongest natural computer, the brain. Models of cortical function must now account for astrocyte activities as well as their relationships with neurons in encoding and manipulation of sensory information. From an engineering viewpoint, astrocytes provide feedback to both presynaptic and postsynaptic neurons to regulate their signaling behaviors. This paper presents a modified neural glial interaction model that allows a convenient digital implementation. This model can reproduce relevant biological astrocyte behaviors, which provide appropriate feedback control in regulating neuronal activities in the central nervous system (CNS). Accordingly, we investigate the feasibility of a digital implementation for a single astrocyte constructed by connecting a two coupled FitzHugh Nagumo (FHN) neuron model to an implementation of the proposed astrocyte model using neuron-astrocyte interactions. Hardware synthesis, physical implementation on FPGA, and theoretical analysis confirm that the proposed neuron astrocyte model, with significantly low hardware cost, can mimic biological behavior such as the regulation of postsynaptic neuron activity and the synaptic transmission mechanisms. </AbstractText>

26390498|t|Probabilistic Boolean Modeling and Analysis Framework for mRNA translation.
26390498|a|<AbstractText Label="null" NlmCategory="UNLABELLED">mRNA translation is a complex process involving the progression of ribosomes on the mRNA, resulting in the synthesis of proteins, and is subject to multiple layers of regulation. This process has been modelled using different formalisms, both stochastic and deterministic. Recently, we introduced a Probabilistic Boolean modelling framework for mRNA translation, which possesses the advantage of tools for numerically exact computation of steady state probability distribution, without requiring simulation. Here we extend this model to incorporate both random sequential and parallel update rules, and demonstrate its effectiveness in various settings, including its flexibility in accommodating additional static and dynamic biological complexities and its role in parameter sensitivity analysis. In these applications, the results from the model analysis match those of TASEP model simulations. Importantly the proposed modelling framework maintains the stochastic aspects of mRNA translation and provides a way to exactly calculate probability distributions, providing additional tools of analysis in this context. Finally the proposed modelling methodology provides an alternative approach to the understanding of the mRNA translation process, by bridging the gap between existing approaches, providing new analysis tools and contributing to a more robust platform for modelling and understanding translation. </AbstractText>

26390497|t|An Unsupervised Graph Based Continuous Word Representation Method for Biomedical Text Mining.
26390497|a|<AbstractText Label="null" NlmCategory="UNLABELLED">In biomedical text mining tasks, distributed word representation has succeeded in capturing semantic regularities, but most of them are shallow-window based models, which are not sufficient for expressing the meaning of words. To represent words using deeper information, we make explicit the semantic regularity to emerge in word relations, including dependency relations and context relations, and propose a novel architecture for computing continuous vector representation by leveraging those relations.The performance of our model is measured on word analogy task and Protein-Protein Interaction Extraction (PPIE) task. Experimental results show that our method performs overall better than other word representation models on word analogy task and have many advantages on biomedical text mining. </AbstractText>

26390496|t|Approximate Graph Edit Distance in Quadratic Time.
26390496|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Graph edit distance is one of the most flexible and general graph matching models available. The major drawback of graph edit distance, however, is its computational complexity that restricts its applicability to graphs of rather small size. Recently the authors of the present paper introduced a general approximation framework for the graph edit distance problem. The basic idea of this specific algorithm is to first compute an optimal assignment of independent local graph structures (including substitutions, deletions, and insertions of nodes and edges). This optimal assignment is complete and consistent with respect to the involved nodes of both graphs and can thus be used to instantly derive an admissible (yet suboptimal) solution for the original graph edit distance problem in O(n3) time. For large scale graphs or graph sets, however, the cubic time complexity may still be too high. Therefore, we propose to use suboptimal algorithms with quadratic rather than cubic time for solving the basic assignment problem. In particular, the present paper introduces five different greedy assignment algorithms in the context of graph edit distance approximation. In an experimental evaluation we show that these methods have great potential for further speeding up the computation of graph edit distance while the approximated distances remain sufficiently accurate for graph based pattern classification. </AbstractText>

26390495|t|Supervised, Unsupervised and Semi-supervised Feature selection: A Review on Gene Selection.
26390495|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Recently, feature selection and dimensionality reduction have become fundamental tools for many data mining tasks, especially for processing high-dimensional data such as gene expression microarray data. Gene expression microarray data comprises up to hundreds of thousands of features with relatively small sample size. Because learning algorithms usually do not work well with this kind of data, a challenge to reduce the data dimensionality arises. A huge number of gene selection techniques are applied to select a subset of relevant features for model construction and to seek for better cancer classification performance. This paper presents the basic taxonomy of feature selection and also reviews the state-of-the-art gene selection methods by grouping the literatures into three categories: supervised, unsupervised and semi-supervised. The comparison of experimental results on top 5 representative gene expression datasets indicates that the classification accuracy of unsupervised and semi-supervised feature selection is competitive with supervised feature selection. </AbstractText>

26390494|t|A Perturbation Based Decomposition of Compound Evoked Potentials for Characterization of Nerve Fiber Size Distributions.
26390494|a|<AbstractText Label="null" NlmCategory="UNLABELLED">The characterization of peripheral nerve fiber distributions, in terms of diameter or velocity, is of clinical significance because information associated with these distributions can be utilized in the differential diagnosis of peripheral neuropathies. Electro-diagnostic techniques can be applied to the investigation of peripheral neuropathies and can yield valuable diagnostic information while being minimally invasive. Nerve conduction velocity studies are single parameter tests that yield no detailed information regarding the characteristics of the population of nerve fibers that contribute to the compound evoked potential. Decomposition of the compound evoked potential, such that the velocity or diameter distribution of the contributing nerve fibers may be determined, is necessary if information regarding the population of contributing nerve fibers is to be ascertained from the electro-diagnostic study. In this work, a perturbation based decomposition of compound evoked potentials is proposed that facilitates determination of the fiber diameter distribution associated with the compound evoked potential. The decomposition is based on representing the single fiber evoked potential, associated with each diameter class, as being perturbed by contributions, of varying degree, from all the other diameter class single fiber evoked potentials. The resultant estimator of the contributing nerve fiber diameter distribution is valid for relatively large separations in diameter classes. It is also useful in situations where the separation between diameter classes is small and the concomitant single fiber evoked potentials are not orthogonal. </AbstractText>

26390493|t|Realtime Reconstruction of an Animating Human Body from a Single Depth Camera.
26390493|a|<AbstractText Label="null" NlmCategory="UNLABELLED">We present a method for realtime reconstruction of an animating human body, which produces a sequence of deforming meshes representing a given performance captured by a single commodity depth camera. We achieve realtime single-view mesh completion by enhancing the parameterized SCAPE model. Our method, which we call Realtime SCAPE, performs full-body reconstruction without the use of markers. In Realtime SCAPE, estimations of body shape parameters and pose parameters, needed for reconstruction, are decoupled. Intrinsic body shape is first precomputed for a given subject, by determining shape parameters with the aid of a body shape database. Subsequently, per-frame pose parameter estimation is performed by means of linear blending skinning (LBS); the problem is decomposed into separately finding skinning weights and transformations. The skinning weights are also determined offline from the body shape database, reducing online reconstruction to simply finding the transformations in LBS. Doing so is formulated as a linear variational problem; carefully designed constraints are used to impose temporal coherence and alleviate artifacts. Experiments demonstrate that our method can produce full-body mesh sequences with high fidelity. </AbstractText>

26390492|t|Spatial Reasoning and Data Displays.
26390492|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Graphics convey numerical information very efficiently, but rely on a different set of mental processes than tabular displays. Here, we present a study relating demographic characteristics and visual skills to perception of graphical lineups. We conclude that lineups are essentially a classification test in a visual domain, and that performance on the lineup protocol is associated with general aptitude, rather than specific tasks such as card rotation and spatial manipulation. We also examine the possibility that specific graphical tasks may be associated with certain visual skills and conclude that more research is necessary to understand which visual skills are required in order to understand certain plot types. </AbstractText>

26390491|t|Improving Bayesian Reasoning: The Effects of Phrasing, Visualization, and Spatial Ability.
26390491|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Decades of research have repeatedly shown that people perform poorly at estimating and understanding conditional probabilities that are inherent in Bayesian reasoning problems. Yet in the medical domain, both physicians and patients make daily, life-critical judgments based on conditional probability. Although there have been a number of attempts to develop more effective ways to facilitate Bayesian reasoning, reports of these findings tend to be inconsistent and sometimes even contradictory. For instance, the reported accuracies for individuals being able to correctly estimate conditional probability range from 6% to 62%. In this work, we show that problem representation can significantly affect accuracies. By controlling the amount of information presented to the user, we demonstrate how text and visualization designs can increase overall accuracies to as high as 77%. Additionally, we found that for users with high spatial ability, our designs can further improve their accuracies to as high as 100%. By and large, our findings provide explanations for the inconsistent reports on accuracy in Bayesian reasoning tasks and show a significant improvement over existing methods. We believe that these findings can have immediate impact on risk communication in health-related fields. </AbstractText>

26390490|t|Visually Comparing Weather Features in Forecasts.
26390490|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Meteorologists process and analyze weather forecasts using visualization in order to examine the behaviors of and relationships among weather features. In this design study conducted with meteorologists in decision support roles, we identified and attempted to address two significant common challenges in weather visualization: the employment of inconsistent and often ineffective visual encoding practices across a wide range of visualizations, and a lack of support for directly visualizing how different weather features relate across an ensemble of possible forecast outcomes. In this work, we present a characterization of the problems and data associated with meteorological forecasting, we propose a set of informed default encoding choices that integrate existing meteorological conventions with effective visualization practice, and we extend a set of techniques as an initial step toward directly visualizing the interactions of multiple features over an ensemble forecast. We discuss the integration of these contributions into a functional prototype tool, and also reflect on the many practical challenges that arise when working with weather data. </AbstractText>

26390489|t|TimeNotes: A Study on Effective Chart Visualization and Interaction Techniques for Time-Series Data.
26390489|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Collecting sensor data results in large temporal data sets which need to be visualized, analyzed, and presented. Onedimensional time-series charts are used, but these present problems when screen resolution is small in comparison to the data. This can result in severe over-plotting, giving rise for the requirement to provide effective rendering and methods to allow interaction with the detailed data. Common solutions can be categorized as multi-scale representations, frequency based, and lens based interaction techniques. In this paper, we comparatively evaluate existing methods, such as Stack Zoom [15] and ChronoLenses [38], giving a graphical overview of each and classifying their ability to explore and interact with data. We propose new visualizations and other extensions to the existing approaches. We undertake and report an empirical study and a field study using these techniques. </AbstractText>

26390488|t|Beyond Memorability: Visualization Recognition and Recall.
26390488|a|<AbstractText Label="null" NlmCategory="UNLABELLED">In this paper we move beyond memorability and investigate how visualizations are recognized and recalled. For this study we labeled a dataset of 393 visualizations and analyzed the eye movements of 33 participants as well as thousands of participantgenerated text descriptions of the visualizations. This allowed us to determine what components of a visualization attract people's attention, and what information is encoded into memory. Our findings quantitatively support many conventional qualitative design guidelines, including that (1) titles and supporting text should convey the message of a visualization, (2) if used appropriately, pictograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message. Importantly, we show that visualizations memorable "at-a-glance" are also capable of effectively conveying the message of the visualization. Thus, a memorable visualization is often also an effective one. </AbstractText>

26390487|t|Probing Projections: Interaction Techniques for Interpreting Arrangements and Errors of Dimensionality Reductions.
26390487|a|<AbstractText Label="null" NlmCategory="UNLABELLED">We introduce a set of integrated interaction techniques to interpret and interrogate dimensionality-reduced data. Projection techniques generally aim to make a high-dimensional information space visible in form of a planar layout. However, the meaning of the resulting data projections can be hard to grasp. It is seldom clear why elements are placed far apart or close together and the inevitable approximation errors of any projection technique are not exposed to the viewer. Previous research on dimensionality reduction focuses on the efficient generation of data projections, interactive customisation of the model, and comparison of different projection techniques. There has been only little research on how the visualization resulting from data projection is interacted with. We contribute the concept of probing as an integrated approach to interpreting the meaning and quality of visualizations and propose a set of interactive methods to examine dimensionality-reduced data as well as the projection itself. The methods let viewers see approximation errors, question the positioning of elements, compare them to each other, and visualize the influence of data dimensions on the projection space. We created a web-based system implementing these methods, and report on findings from an evaluation with data analysts using the prototype to examine multidimensional datasets. </AbstractText>

26390486|t|AmbiguityVis: Visualization of Ambiguity in Graph Layouts.
26390486|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Node-link diagrams provide an intuitive way to explore networks and have inspired a large number of automated graph layout strategies that optimize aesthetic criteria. However, any particular drawing approach cannot fully satisfy all these criteria simultaneously, producing drawings with visual ambiguities that can impede the understanding of network structure. To bring attention to these potentially problematic areas present in the drawing, this paper presents a technique that highlights common types of visual ambiguities: ambiguous spatial relationships between nodes and edges, visual overlap between community structures, and ambiguity in edge bundling and metanodes. Metrics, including newly proposed metrics for abnormal edge lengths, visual overlap in community structures and node/edge aggregation, are proposed to quantify areas of ambiguity in the drawing. These metrics and others are then displayed using a heatmap-based visualization that provides visual feedback to developers of graph drawing and visualization approaches, allowing them to quickly identify misleading areas. The novel metrics and the heatmap-based visualization allow a user to explore ambiguities in graph layouts from multiple perspectives in order to make reasonable graph layout choices. The effectiveness of the technique is demonstrated through case studies and expert reviews. </AbstractText>

26390485|t|Beyond Weber's Law: A Second Look at Ranking Visualizations of Correlation.
26390485|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Models of human perception - including perceptual "laws" - can be valuable tools for deriving visualization design recommendations. However, it is important to assess the explanatory power of such models when using them to inform design. We present a secondary analysis of data previously used to rank the effectiveness of bivariate visualizations for assessing correlation (measured with Pearson's r) according to the well-known Weber-Fechner Law. Beginning with the model of Harrison et al. [1], we present a sequence of refinements including incorporation of individual differences, log transformation, censored regression, and adoption of Bayesian statistics. Our model incorporates all observations dropped from the original analysis, including data near ceilings caused by the data collection process and entire visualizations dropped due to large numbers of observations worse than chance. This model deviates from Weber's Law, but provides improved predictive accuracy and generalization. Using Bayesian credibility intervals, we derive a partial ranking that groups visualizations with similar performance, and we give precise estimates of the difference in performance between these groups. We find that compared to other visualizations, scatterplots are unique in combining low variance between individuals and high precision on both positively- and negatively- correlated data. We conclude with a discussion of the value of data sharing and replication, and share implications for modeling similar experimental data. </AbstractText>

26390484|t|A Linguistic Approach to Categorical Color Assignment for Data Visualization.
26390484|a|<AbstractText Label="null" NlmCategory="UNLABELLED">When data categories have strong color associations, it is useful to use these semantically meaningful concept-color associations in data visualizations. In this paper, we explore how linguistic information about the terms defining the data can be used to generate semantically meaningful colors. To do this effectively, we need first to establish that a term has a strong semantic color association, then discover which color or colors express it. Using co-occurrence measures of color name frequencies from Google n-grams, we define a measure for colorability that describes how strongly associated a given term is to any of a set of basic color terms. We then show how this colorability score can be used with additional semantic analysis to rank and retrieve a representative color from Google Images. Alternatively, we use symbolic relationships defined by WordNet to select identity colors for categories such as countries or brands. To create visually distinct color palettes, we use k-means clustering to create visually distinct sets, iteratively reassigning terms with multiple basic color associations as needed. This can be additionally constrained to use colors only in a predefined palette. </AbstractText>

26390483|t|HOLA: Human-like Orthogonal Network Layout.
26390483|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Over the last 50 years a wide variety of automatic network layout algorithms have been developed. Some are fast heuristic techniques suitable for networks with hundreds of thousands of nodes while others are multi-stage frameworks for higher-quality layout of smaller networks. However, despite decades of research currently no algorithm produces layout of comparable quality to that of a human. We give a new "human-centred" methodology for automatic network layout algorithm design that is intended to overcome this deficiency. User studies are first used to identify the aesthetic criteria algorithms should encode, then an algorithm is developed that is informed by these criteria and finally, a follow-up study evaluates the algorithm output. We have used this new methodology to develop an automatic orthogonal network layout method, HOLA, that achieves measurably better (by user study) layout than the best available orthogonal layout algorithm and which produces layouts of comparable quality to those produced by hand. </AbstractText>

26390482|t|TimeSpan: Using Visualization to Explore Temporal Multi-Dimensional Data of Stroke Patients.
26390482|a|<AbstractText Label="null" NlmCategory="UNLABELLED">We present TimeSpan, an exploratory visualization tool designed to gain a better understanding of the temporal aspects of the stroke treatment process. Working with stroke experts, we seek to provide a tool to help improve outcomes for stroke victims. Time is of critical importance in the treatment of acute ischemic stroke patients. Every minute that the artery stays blocked, an estimated 1.9 million neurons and 12 km of myelinated axons are destroyed. Consequently, there is a critical need for efficiency of stroke treatment processes. Optimizing time to treatment requires a deep understanding of interval times. Stroke health care professionals must analyze the impact of procedures, events, and patient attributes on time-ultimately, to save lives and improve quality of life after stroke. First, we interviewed eight domain experts, and closely collaborated with two of them to inform the design of TimeSpan. We classify the analytical tasks which a visualization tool should support and extract design goals from the interviews and field observations. Based on these tasks and the understanding gained from the collaboration, we designed TimeSpan, a web-based tool for exploring multi-dimensional and temporal stroke data. We describe how TimeSpan incorporates factors from stacked bar graphs, line charts, histograms, and a matrix visualization to create an interactive hybrid view of temporal data. From feedback collected from domain experts in a focus group session, we reflect on the lessons we learned from abstracting the tasks and iteratively designing TimeSpan. </AbstractText>

26390481|t|Intuitive Exploration of Volumetric Data Using Dynamic Galleries.
26390481|a|<AbstractText Label="null" NlmCategory="UNLABELLED">In this work we present a volume exploration method designed to be used by novice users and visitors to science centers and museums. The volumetric digitalization of artifacts in museums is of rapidly increasing interest as enhanced user experience through interactive data visualization can be achieved. This is, however, a challenging task since the vast majority of visitors are not familiar with the concepts commonly used in data exploration, such as mapping of visual properties from values in the data domain using transfer functions. Interacting in the data domain is an effective way to filter away undesired information but it is difficult to predict where the values lie in the spatial domain. In this work we make extensive use of dynamic previews instantly generated as the user explores the data domain. The previews allow the user to predict what effect changes in the data domain will have on the rendered image without being aware that visual parameters are set in the data domain. Each preview represents a subrange of the data domain where overview and details are given on demand through zooming and panning. The method has been designed with touch interfaces as the target platform for interaction. We provide a qualitative evaluation performed with visitors to a science center to show the utility of the approach. </AbstractText>

26390480|t|Real-Time Molecular Visualization Supporting Diffuse Interreflections and Ambient Occlusion.
26390480|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Today molecular simulations produce complex data sets capturing the interactions of molecules in detail. Due to the complexity of this time-varying data, advanced visualization techniques are required to support its visual analysis. Current molecular visualization techniques utilize ambient occlusion as a global illumination approximation to improve spatial comprehension. Besides these shadow-like effects, interreflections are also known to improve the spatial comprehension of complex geometric structures. Unfortunately, the inherent computational complexity of interreflections would forbid interactive exploration, which is mandatory in many scenarios dealing with static and time-varying data. In this paper, we introduce a novel analytic approach for capturing interreflections of molecular structures in real-time. By exploiting the knowledge of the underlying space filling representations, we are able to reduce the required parameters and can thus apply symbolic regression to obtain an analytic expression for interreflections. We show how to obtain the data required for the symbolic regression analysis, and how to exploit our analytic solution to enhance interactive molecular visualizations. </AbstractText>

26390479|t|Multi-field Pattern Matching based on Sparse Feature Sampling.
26390479|a|<AbstractText Label="null" NlmCategory="UNLABELLED">We present an approach to pattern matching in 3D multi-field scalar data. Existing pattern matching algorithms work on single scalar or vector fields only, yet many numerical simulations output multi-field data where only a joint analysis of multiple fields describes the underlying phenomenon fully. Our method takes this into account by bundling information from multiple fields into the description of a pattern. First, we extract a sparse set of features for each 3D scalar field using the 3D SIFT algorithm (Scale-Invariant Feature Transform). This allows for a memory-saving description of prominent features in the data with invariance to translation, rotation, and scaling. Second, the user defines a pattern as a set of SIFT features in multiple fields by e.g. brushing a region of interest. Third, we locate and rank matching patterns in the entire data set. Experiments show that our algorithm is efficient in terms of required memory and computational efforts. </AbstractText>

26390478|t|Sketching Designs Using the Five Design-Sheet Methodology.
26390478|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Sketching designs has been shown to be a useful way of planning and considering alternative solutions. The use of lofidelity prototyping, especially paper-based sketching, can save time, money and converge to better solutions more quickly. However, this design process is often viewed to be too informal. Consequently users do not know how to manage their thoughts and ideas (to first think divergently, to then finally converge on a suitable solution). We present the Five Design Sheet (FdS) methodology. The methodology enables users to create information visualization interfaces through lo-fidelity methods. Users sketch and plan their ideas, helping them express different possibilities, think through these ideas to consider their potential effectiveness as solutions to the task (sheet 1); they create three principle designs (sheets 2,3 and 4); before converging on a final realization design that can then be implemented (sheet 5). In this article, we present (i) a review of the use of sketching as a planning method for visualization and the benefits of sketching, (ii) a detailed description of the Five Design Sheet (FdS) methodology, and (iii) an evaluation of the FdS using the System Usability Scale, along with a case-study of its use in industry and experience of its use in teaching. </AbstractText>

26390477|t|High-Quality Ultra-Compact Grid Layout of Grouped Networks.
26390477|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Prior research into network layout has focused on fast heuristic techniques for layout of large networks, or complex multi-stage pipelines for higher quality layout of small graphs. Improvements to these pipeline techniques, especially for orthogonal-style layout, are difficult and practical results have been slight in recent years. Yet, as discussed in this paper, there remain significant issues in the quality of the layouts produced by these techniques, even for quite small networks. This is especially true when layout with additional grouping constraints is required. The first contribution of this paper is to investigate an ultra-compact, grid-like network layout aesthetic that is motivated by the grid arrangements that are used almost universally by designers in typographical layout. Since the time when these heuristic and pipeline-based graph-layout methods were conceived, generic technologies (MIP, CP and SAT) for solving combinatorial and mixed-integer optimization problems have improved massively. The second contribution of this paper is to reassess whether these techniques can be used for high-quality layout of small graphs. While they are fast enough for graphs of up to 50 nodes we found these methods do not scale up. Our third contribution is a large-neighborhood search meta-heuristic approach that is scalable to larger networks. </AbstractText>

26390476|t|Streamline Variability Plots for Characterizing the Uncertainty in Vector Field Ensembles.
26390476|a|<AbstractText Label="null" NlmCategory="UNLABELLED">We present a new method to visualize from an ensemble of flow fields the statistical properties of streamlines passing through a selected location. We use principal component analysis to transform the set of streamlines into a low-dimensional Euclidean space. In this space the streamlines are clustered into major trends, and each cluster is in turn approximated by a multivariate Gaussian distribution. This yields a probabilistic mixture model for the streamline distribution, from which confidence regions can be derived in which the streamlines are most likely to reside. This is achieved by transforming the Gaussian random distributions from the low-dimensional Euclidean space into a streamline distribution that follows the statistical model, and by visualizing confidence regions in this distribution via iso-contours. We further make use of the principal component representation to introduce a new concept of streamline-median, based on existing median concepts in multidimensional Euclidean spaces. We demonstrate the potential of our method in a number of real-world examples, and we compare our results to alternative clustering approaches for particle trajectories as well as curve boxplots. </AbstractText>

26390475|t|Cluster Analysis of Vortical Flow in Simulations of Cerebral Aneurysm Hemodynamics.
26390475|a|<AbstractText Label="null" NlmCategory="UNLABELLED">Computational fluid dynamic (CFD) simulations of blood flow provide new insights into the hemodynamics of vascular pathologies such as cerebral aneurysms. Understanding the relations between hemodynamics and aneurysm initiation, progression, and risk of rupture is crucial in diagnosis and treatment. Recent studies link the existence of vortices in the blood flow pattern to aneurysm rupture and report observations of embedded vortices - a larger vortex encloses a smaller one flowing in the opposite direction - whose implications are unclear. We present a clustering-based approach for the visual analysis of vortical flow in simulated cerebral aneurysm hemodynamics. We show how embedded vortices develop at saddle-node bifurcations on vortex core lines and convey the participating flow at full manifestation of the vortex by a fast and smart grouping of streamlines and the visualization of group representatives. The grouping result may be refined based on spectral clustering generating a more detailed visualization of the flow pattern, especially further off the core lines. We aim at supporting CFD engineers researching the biological implications of embedded vortices. </AbstractText>

26390474|t|CAST: Effective and Efficient User Interaction for Context-Aware Selection in 3D Particle Clouds.
26390474|a|<AbstractText Label="null" NlmCategory="UNLABELLED">We present a family of three interactive Context-Aware Selection Techniques (CAST) for the analysis of large 3D particle datasets. For these datasets, spatial selection is an essential prerequisite to many other analysis tasks. Traditionally, such interactive target selection has been particularly challenging when the data subsets of interest were implicitly defined in the form of complicated structures of thousands of particles. Our new techniques SpaceCast, TraceCast, and PointCast improve usability and speed of spatial selection in point clouds through novel context-aware algorithms. They are able to infer a user's subtle selection intention from gestural input, can deal with complex situations such as partially occluded point clusters or multiple cluster layers, and can all be fine-tuned after the selection interaction has been completed. Together, they provide an effective and efficient tool set for the fast exploratory analysis of large datasets. In addition to presenting Cast, we report on a formal user study that compares our new techniques not only to each other but also to existing state-of-the-art selection methods. Our results show that Cast family members are virtually always faster than existing methods without tradeoffs in accuracy. In addition, qualitative feedback shows that PointCast and TraceCast were strongly favored by our participants for intuitiveness and efficiency. </AbstractText>

26390473|t|Suggested Interactivity: Seeking Perceived Affordances for Information Visualization.
26390473|a|<AbstractText Label="null" NlmCategory="UNLABELLED">In this article, we investigate methods for suggesting the interactivity of online visualizations embedded with text. We first assess the need for such methods by conducting three initial experiments on Amazon's Mechanical Turk. We then present a design space for Suggested Interactivity (i. e., visual cues used as perceived affordances-SI), based on a survey of 382 HTML5 and visualization websites. Finally, we assess the effectiveness of three SI cues we designed for suggesting the interactivity of bar charts embedded with text. Our results show that only one cue (SI3) was successful in inciting participants to interact with the visualizations, and we hypothesize this is because this particular cue provided feedforward. </AbstractText>

26390472|t|Rotation Invariant Vortices for Flow Visualization.
26390472|a|<AbstractText Label="null" NlmCategory="UNLABELLED">We propose a new class of vortex definitions for flows that are induced by rotating mechanical parts, such as stirring devices, helicopters, hydrocyclones, centrifugal pumps, or ventilators. Instead of a Galilean invariance, we enforce a rotation invariance, i.e., the invariance of a vortex under a uniform-speed rotation of the underlying coordinate system around a fixed axis. We provide a general approach to transform a Galilean invariant vortex concept to a rotation invariant one by simply adding a closed form matrix to the Jacobian. In particular, we present rotation invariant versions of the well-known Sujudi-Haimes, Lambda-2, and Q vortex criteria. We apply them to a number of artificial and real rotating flows, showing that for these cases rotation invariant vortices give better results than their Galilean invariant counterparts. </AbstractText>

